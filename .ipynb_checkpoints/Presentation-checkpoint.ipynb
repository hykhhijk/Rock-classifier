{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "551ba211",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-09T07:17:48.555374Z",
     "start_time": "2022-05-09T07:17:44.576652Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import cv2\n",
    "import joblib\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import albumentations as A\n",
    "import time\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe12ae97",
   "metadata": {},
   "source": [
    "# Import data and split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b89844",
   "metadata": {},
   "source": [
    "## cv2를 이용한 최근방 이웃, Lanczo, keras의 보간법을 사용하여<br> 같은 데이터를 3번 읽은 뒤 random_state를 고정한 상태에서 train과 test로 나누어 더함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7cf0d409",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-09T07:30:08.505566Z",
     "start_time": "2022-05-09T07:30:07.962978Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = []\n",
    "X_test = []\n",
    "y_train = []\n",
    "y_test =[]\n",
    "\n",
    "X = joblib.load(\"CV2_float32.pkl\")\n",
    "y = joblib.load(\"label.pkl\")\n",
    "X_train_, X_test_, y_train_, y_test_ = train_test_split(X, y, stratify=y, shuffle=True, random_state=42)\n",
    "X_train.append(X_train_), y_train.append(y_train_), X_test.append(X_test_), y_test.append(y_test_)\n",
    "\n",
    "X = joblib.load(\"converted_img.pkl\")\n",
    "X_train_, X_test_, y_train_, y_test_ = train_test_split(X, y, stratify=y, shuffle=True, random_state=42)\n",
    "X_train.append(X_train_), y_train.append(y_train_), X_test.append(X_test_), y_test.append(y_test_)\n",
    "\n",
    "X = joblib.load(\"CV2_LANCZO.pkl\")\n",
    "X_train_, X_test_, y_train_, y_test_ = train_test_split(X, y, stratify=y, shuffle=True, random_state=42)\n",
    "X_train.append(X_train_), y_train.append(y_train_), X_test.append(X_test_), y_test.append(y_test_)\n",
    "\n",
    "X_train = np.float32(X_train)\n",
    "X_test = np.float32(X_test)\n",
    "y_train = np.float32(y_train)\n",
    "y_test = np.float32(y_test)\n",
    "X_train = X_train.reshape(-1, 32, 32, 3)\n",
    "X_test = X_test.reshape(-1, 32, 32, 3)\n",
    "y_train = y_train.reshape(-1)\n",
    "y_test = y_test.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1098bc2e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-09T07:30:25.383824Z",
     "start_time": "2022-05-09T07:30:25.373835Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneHotEncoder(sparse=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = OneHotEncoder(sparse=False)\n",
    "encoder.fit(y_train.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d046e01",
   "metadata": {},
   "source": [
    "인코더는 증강 이후 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466faa7a",
   "metadata": {},
   "source": [
    "# Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6bcd5e",
   "metadata": {},
   "source": [
    "## 50%확률로 수직, 수평으로 회전하는 증강 사용하여 10번 반복하여 데이터셋 증강"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca9e12f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-09T07:30:08.861965Z",
     "start_time": "2022-05-09T07:30:08.847614Z"
    }
   },
   "outputs": [],
   "source": [
    "transform_fiip = A.Compose([\n",
    "    A.Resize(32, 32),\n",
    "    A.HorizontalFlip(p=0.5), \n",
    "    A.VerticalFlip(p=0.5),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1350a3d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-09T07:30:11.637456Z",
     "start_time": "2022-05-09T07:30:11.625602Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_train_data(augment):\n",
    "    img_list = []\n",
    "    label_list = []\n",
    "    for _ in range(10):\n",
    "        for i in range(len(X_train)):\n",
    "            img_list.append(augment(image=X_train[i])[\"image\"])\n",
    "            label_list.append(y_train[i])\n",
    "    img_list = np.float32(img_list)\n",
    "    img_list = img_list.reshape(-1, 32, 32, 3)\n",
    "    label_list = np.float32(label_list)\n",
    "    return [img_list, label_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8aa732a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-09T07:30:49.625827Z",
     "start_time": "2022-05-09T07:30:45.058669Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, y_train = make_train_data(transform_fiip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1616601d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-09T07:31:21.611583Z",
     "start_time": "2022-05-09T07:31:21.589614Z"
    }
   },
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder(sparse=False)\n",
    "encoder.fit(y_train.reshape(-1, 1))\n",
    "y_train = encoder.transform(y_train.reshape(-1, 1))\n",
    "y_test = encoder.transform(y_test.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5643af5c",
   "metadata": {},
   "source": [
    "# Train model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de48e49f",
   "metadata": {},
   "source": [
    "## 조기종료: 5에포크, 학습률 감소: 3에포크 수렴 못할 시 0.001->0.00001까지 1/2<br>모델은 같은 크기(32, 32, 3)이미지에 사용된 Resnet20형태를 참고하여 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "08674411",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-09T07:37:29.556440Z",
     "start_time": "2022-05-09T07:37:29.541479Z"
    }
   },
   "outputs": [],
   "source": [
    "es = keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", patience=5, mode=\"max\")\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5,patience=3, min_lr=0.00001, verbose=1)\n",
    "mc = keras.callbacks.ModelCheckpoint(filepath=\"./best_model.h5\",monitor=\"val_loss\", save_best_only=True, mode=\"min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "15849f46",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-09T07:54:16.845692Z",
     "start_time": "2022-05-09T07:37:35.870319Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5119/5119 [==============================] - 59s 11ms/step - loss: 0.2364 - accuracy: 0.9062 - val_loss: 0.1603 - val_accuracy: 0.9524\n",
      "Epoch 2/100\n",
      "   6/5119 [..............................] - ETA: 58s - loss: 0.0744 - accuracy: 0.9792 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hjhhi\\anaconda3\\envs\\python3.9.0\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\generic_utils.py:494: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  warnings.warn('Custom mask layers require a config and must override '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5119/5119 [==============================] - 58s 11ms/step - loss: 0.0570 - accuracy: 0.9795 - val_loss: 0.4432 - val_accuracy: 0.8722\n",
      "Epoch 3/100\n",
      "5119/5119 [==============================] - 58s 11ms/step - loss: 0.0353 - accuracy: 0.9876 - val_loss: 1.2196 - val_accuracy: 0.7502\n",
      "Epoch 4/100\n",
      "5119/5119 [==============================] - 58s 11ms/step - loss: 0.0259 - accuracy: 0.9910 - val_loss: 0.0159 - val_accuracy: 0.9943\n",
      "Epoch 5/100\n",
      "5119/5119 [==============================] - 59s 11ms/step - loss: 0.0204 - accuracy: 0.9931 - val_loss: 0.7698 - val_accuracy: 0.8451\n",
      "Epoch 6/100\n",
      "5119/5119 [==============================] - 56s 11ms/step - loss: 0.0175 - accuracy: 0.9941 - val_loss: 0.0117 - val_accuracy: 0.9947\n",
      "Epoch 7/100\n",
      "5119/5119 [==============================] - 57s 11ms/step - loss: 0.0132 - accuracy: 0.9955 - val_loss: 0.0067 - val_accuracy: 0.9974\n",
      "Epoch 8/100\n",
      "5119/5119 [==============================] - 59s 12ms/step - loss: 0.0123 - accuracy: 0.9959 - val_loss: 0.3580 - val_accuracy: 0.9179\n",
      "Epoch 9/100\n",
      "5119/5119 [==============================] - 57s 11ms/step - loss: 0.0103 - accuracy: 0.9966 - val_loss: 0.0248 - val_accuracy: 0.9930\n",
      "Epoch 10/100\n",
      "5119/5119 [==============================] - 57s 11ms/step - loss: 0.0095 - accuracy: 0.9968 - val_loss: 0.0185 - val_accuracy: 0.9956\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 11/100\n",
      "5119/5119 [==============================] - 56s 11ms/step - loss: 0.0039 - accuracy: 0.9988 - val_loss: 1.1349 - val_accuracy: 0.8231\n",
      "Epoch 12/100\n",
      "5119/5119 [==============================] - 56s 11ms/step - loss: 0.0033 - accuracy: 0.9989 - val_loss: 0.0026 - val_accuracy: 0.9991\n",
      "Epoch 13/100\n",
      "5119/5119 [==============================] - 56s 11ms/step - loss: 0.0031 - accuracy: 0.9990 - val_loss: 0.0023 - val_accuracy: 0.9991\n",
      "Epoch 14/100\n",
      "5119/5119 [==============================] - 56s 11ms/step - loss: 0.0031 - accuracy: 0.9990 - val_loss: 0.0149 - val_accuracy: 0.9945\n",
      "Epoch 15/100\n",
      "5119/5119 [==============================] - 57s 11ms/step - loss: 0.0070 - accuracy: 0.9977 - val_loss: 0.0121 - val_accuracy: 0.9974\n",
      "Epoch 16/100\n",
      "5119/5119 [==============================] - 57s 11ms/step - loss: 0.0036 - accuracy: 0.9988 - val_loss: 0.1259 - val_accuracy: 0.9692oss: 0.0036 - accuracy: 0. - ETA: 0s\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 17/100\n",
      "5119/5119 [==============================] - 54s 11ms/step - loss: 0.0020 - accuracy: 0.9994 - val_loss: 0.0262 - val_accuracy: 0.9908\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "Input \u001b[1;32mIn [22]\u001b[0m, in \u001b[0;36m<cell line: 61>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     57\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     58\u001b[0m optimizer\u001b[38;5;241m=\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m), metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     60\u001b[0m history\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, validation_data\u001b[38;5;241m=\u001b[39m(X_test, y_test),callbacks\u001b[38;5;241m=\u001b[39m[es, reduce_lr, mc],epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m---> 61\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain: \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValid: \u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m.\u001b[39mevaluate(X_test, y_test))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\python3.9.0\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1464\u001b[0m, in \u001b[0;36mModel.evaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m   1461\u001b[0m   data_handler \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eval_data_handler\n\u001b[0;32m   1462\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1463\u001b[0m   \u001b[38;5;66;03m# Creates a `tf.data.Dataset` and handles batch and epoch iteration.\u001b[39;00m\n\u001b[1;32m-> 1464\u001b[0m   data_handler \u001b[38;5;241m=\u001b[39m \u001b[43mdata_adapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_data_handler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1465\u001b[0m \u001b[43m      \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1466\u001b[0m \u001b[43m      \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1467\u001b[0m \u001b[43m      \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1468\u001b[0m \u001b[43m      \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1469\u001b[0m \u001b[43m      \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1470\u001b[0m \u001b[43m      \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1471\u001b[0m \u001b[43m      \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1472\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1473\u001b[0m \u001b[43m      \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1474\u001b[0m \u001b[43m      \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1475\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1476\u001b[0m \u001b[43m      \u001b[49m\u001b[43msteps_per_execution\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_steps_per_execution\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1478\u001b[0m \u001b[38;5;66;03m# Container that configures and calls `tf.keras.Callback`s.\u001b[39;00m\n\u001b[0;32m   1479\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(callbacks, callbacks_module\u001b[38;5;241m.\u001b[39mCallbackList):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\python3.9.0\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py:1394\u001b[0m, in \u001b[0;36mget_data_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1392\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cluster_coordinator\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1393\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _ClusterCoordinatorDataHandler(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m-> 1394\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataHandler(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\python3.9.0\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py:1149\u001b[0m, in \u001b[0;36mDataHandler.__init__\u001b[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[0;32m   1146\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_steps_per_execution_value \u001b[38;5;241m=\u001b[39m steps_per_execution\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m   1148\u001b[0m adapter_cls \u001b[38;5;241m=\u001b[39m select_data_adapter(x, y)\n\u001b[1;32m-> 1149\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_adapter \u001b[38;5;241m=\u001b[39m \u001b[43madapter_cls\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1150\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1151\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1153\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1154\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1155\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1157\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1158\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1159\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1160\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdistribution_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mds_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_strategy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1161\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1163\u001b[0m strategy \u001b[38;5;241m=\u001b[39m ds_context\u001b[38;5;241m.\u001b[39mget_strategy()\n\u001b[0;32m   1165\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\python3.9.0\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py:243\u001b[0m, in \u001b[0;36mTensorLikeDataAdapter.__init__\u001b[1;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    233\u001b[0m              x,\n\u001b[0;32m    234\u001b[0m              y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    240\u001b[0m              shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    241\u001b[0m              \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    242\u001b[0m   \u001b[38;5;28msuper\u001b[39m(TensorLikeDataAdapter, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(x, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 243\u001b[0m   x, y, sample_weights \u001b[38;5;241m=\u001b[39m \u001b[43m_process_tensorlike\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weights\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    244\u001b[0m   sample_weight_modes \u001b[38;5;241m=\u001b[39m broadcast_sample_weight_modes(\n\u001b[0;32m    245\u001b[0m       sample_weights, sample_weight_modes)\n\u001b[0;32m    247\u001b[0m   \u001b[38;5;66;03m# If sample_weights are not specified for an output use 1.0 as weights.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\python3.9.0\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py:1042\u001b[0m, in \u001b[0;36m_process_tensorlike\u001b[1;34m(inputs)\u001b[0m\n\u001b[0;32m   1039\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _scipy_sparse_to_sparse_tensor(x)\n\u001b[0;32m   1040\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[1;32m-> 1042\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[43mnest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_convert_numpy_and_scipy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1043\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m nest\u001b[38;5;241m.\u001b[39mlist_to_tuple(inputs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\python3.9.0\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:869\u001b[0m, in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    865\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[0;32m    866\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[1;32m--> 869\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [func(\u001b[38;5;241m*\u001b[39mx) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[0;32m    870\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\python3.9.0\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:869\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    865\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[0;32m    866\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[1;32m--> 869\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[0;32m    870\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\python3.9.0\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py:1037\u001b[0m, in \u001b[0;36m_process_tensorlike.<locals>._convert_numpy_and_scipy\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m   1035\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(x\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype, np\u001b[38;5;241m.\u001b[39mfloating):\n\u001b[0;32m   1036\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m backend\u001b[38;5;241m.\u001b[39mfloatx()\n\u001b[1;32m-> 1037\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensor_v2_with_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1038\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m _is_scipy_sparse(x):\n\u001b[0;32m   1039\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _scipy_sparse_to_sparse_tensor(x)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\python3.9.0\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\python3.9.0\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:106\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m    104\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[0;32m    105\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m--> 106\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mInternalError\u001b[0m: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized."
     ]
    }
   ],
   "source": [
    "dr_ratio=0.2\n",
    "\n",
    "Input = keras.layers.Input(shape=X_train[0].shape)\n",
    "\n",
    "x = keras.layers.Conv2D(16, kernel_size=3, activation=\"relu\",kernel_initializer=\"he_normal\", padding=\"same\")(Input)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "x = keras.layers.MaxPool2D(2)(x)\n",
    "\n",
    "x = keras.layers.Dropout(dr_ratio)(x)\n",
    "\n",
    "shortcut =x\n",
    "x = keras.layers.Conv2D(32, kernel_size=3, activation=\"relu\",kernel_initializer=\"he_normal\", padding=\"same\")(x)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "x = keras.layers.Conv2D(32, kernel_size=3, activation=\"relu\",kernel_initializer=\"he_normal\", padding=\"same\")(x)\n",
    "# x = keras.layers.BatchNormalization()(x)\n",
    "# x = keras.layers.Conv2D(32, kernel_size=3, activation=\"relu\",kernel_initializer=\"he_normal\", padding=\"same\")(x)\n",
    "shortcut = keras.layers.Conv2D(32, kernel_size=5, activation=\"relu\",kernel_initializer=\"he_normal\", padding=\"same\")(shortcut)\n",
    "x = keras.layers.Add()([x, shortcut])\n",
    "x   = keras.layers.BatchNormalization()(x)\n",
    "x = keras.layers.MaxPool2D(2)(x)\n",
    "\n",
    "\n",
    "x = keras.layers.Dropout(dr_ratio)(x)\n",
    "\n",
    "\n",
    "shortcut =x\n",
    "x = keras.layers.Conv2D(64, kernel_size=3, activation=\"relu\",kernel_initializer=\"he_normal\", padding=\"same\")(x)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "x = keras.layers.Conv2D(64, kernel_size=3, activation=\"relu\",kernel_initializer=\"he_normal\", padding=\"same\")(x)\n",
    "# x = keras.layers.BatchNormalization()(x)\n",
    "# x = keras.layers.Conv2D(64, kernel_size=3, activation=\"relu\",kernel_initializer=\"he_normal\", padding=\"same\")(x)\n",
    "shortcut = keras.layers.Conv2D(64, kernel_size=3, activation=\"relu\",kernel_initializer=\"he_normal\", padding=\"same\")(shortcut)\n",
    "x = keras.layers.Add()([x, shortcut])\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "x = keras.layers.MaxPool2D(2)(x)\n",
    "\n",
    "x = keras.layers.Dropout(dr_ratio)(x)\n",
    "\n",
    "shortcut =x\n",
    "x = keras.layers.Conv2D(128, kernel_size=3, activation=\"relu\",kernel_initializer=\"he_normal\", padding=\"same\")(x)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "x = keras.layers.Conv2D(128, kernel_size=3, activation=\"relu\",kernel_initializer=\"he_normal\", padding=\"same\")(x)\n",
    "# x = keras.layers.BatchNormalization()(x)\n",
    "# x = keras.layers.Conv2D(128, kernel_size=3, activation=\"relu\",kernel_initializer=\"he_normal\", padding=\"same\")(x)\n",
    "shortcut = keras.layers.Conv2D(128, kernel_size=3, activation=\"relu\",kernel_initializer=\"he_normal\", padding=\"same\")(shortcut)\n",
    "x = keras.layers.Add()([x, shortcut])\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "x = keras.layers.MaxPool2D(2)(x)\n",
    "\n",
    "x = keras.layers.Dropout(dr_ratio)(x)\n",
    "\n",
    "\n",
    "x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "Output = keras.layers.Dense(4, activation=\"softmax\")(x)\n",
    "\n",
    "model = keras.models.Model(inputs = Input, outputs = Output)\n",
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "optimizer=keras.optimizers.Adam(learning_rate=0.001), metrics=[\"accuracy\"])\n",
    "\n",
    "history=model.fit(X_train, y_train, validation_data=(X_test, y_test),callbacks=[es, reduce_lr, mc],epochs = 100)\n",
    "print(\"Train: \", model.evaluate(X_train, y_train))\n",
    "print(\"Valid: \", model.evaluate(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b177c96b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "=python3.9.0",
   "language": "python",
   "name": "python3.9.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
